


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from collections import Counter
from sklearn.linear_model import LinearRegression


csv_file = "../Data/R12-over-10-copies.csv"
df = pd.read_csv(csv_file)
df.head()


df.shape


sum(df.iloc[:, 0].isnull())


sum(df.iloc[:, 1].isnull())


df.dtypes


# check base type proportions
def base_count(text):
    return dict(Counter(text))   


df['Base Counts'] = df['Sequence'].apply(base_count)
# df['Base Counts'][1]['A']
df['A'] = df['Base Counts'].astype(object).apply(lambda x: x.get('A', np.nan))
df['C'] = df['Base Counts'].astype(object).apply(lambda x: x.get('C', np.nan))
df['T'] = df['Base Counts'].astype(object).apply(lambda x: x.get('G', np.nan))
df['G'] = df['Base Counts'].astype(object).apply(lambda x: x.get('T', np.nan))


df.head().iloc[:, 3:]


df['Length'] = df['Sequence'].apply(len)


sum((np.sum(df[['A', 'C', 'T', 'G']], axis=1) == df['Length']).isnull()) # Checks base counts add up to length of sequence


allowed_keys = {'A', 'C', 'T', 'G'} # Checks that there are no weird nonstandard bases
df['Nonstandard base'] = df['Base Counts'].astype(object).apply(lambda x: not set(x.keys()).issubset(allowed_keys))
np.sum(df['Nonstandard base'])


df.head()


df['Length'].value_counts().sort_values(ascending=False)





v_df = df # Save a verbose version of df
df = df[['Sequence', 'Length', 'Count', 'A', 'C', 'T', 'G']]
df.head()


for col in ['A', 'C', 'T', 'G']:
    df.loc[:, col] = df[col]/df['Length']


df.head()


df['Count'].describe()





fig, axes = plt.subplots(2, 2, figsize=(10,8))

df.plot(kind='scatter', x='Count', y='A', ax=axes[0, 0], title='Count vs A', color='red')
df.plot(kind='scatter', x='Count', y='C', ax=axes[0, 1], title='Count vs C', color='green')
df.plot(kind='scatter', x='Count', y='G', ax=axes[1, 0], title='Count vs G', color='orange')
df.plot(kind='scatter', x='Count', y='T', ax=axes[1, 1], title='Count vs T', color='purple')

plt.tight_layout()
plt.show()


# HERE
# make histogram and then examine relationships





fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharey=True)

# Compute global max variance for consistency
max_variance = max(df[col].var() for col in base_columns)

for col, ax in zip(base_columns, axes.flatten()):
    grouped_variance = df.groupby('Count')[col].var()
    grouped_variance.plot(kind='bar', ax=ax, color=base_colors[col])
    ax.set_title(f'Variance of {col} by Count')
    ax.set_xlabel('Count')
    ax.set_ylabel(f'Variance of {col}')
    ax.set_ylim(0, max_variance * 1.1)
    
plt.tight_layout()
plt.show()



